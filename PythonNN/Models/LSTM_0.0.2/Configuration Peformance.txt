Confusion Matrix:
 [[11  2  4  0  0  0  0  1  0  0  1]
 [ 1  5 12  0  0  0  0  0  0  0  3]
 [ 2  6  6  0  0  0  0  0  0  0  5]
 [ 0  0  1  4  5  2  1  1  1  0  0]
 [ 1  0  0  9  4  3  2  1  2  0  0]
 [ 0  0  0  1  1  9  2  2  1  0  0]
 [ 0  0  0  3  0  4  7  2  1  0  0]
 [ 1  0  0  3  0  1  2  8  7  0  0]
 [ 1  0  0  4  0  2  0  7  7  0  0]
 [ 0  0  0  0  0  0  0  0  0  0 19]
 [ 0  0  0  1  0  0  0  0  0  0 12]]

Classification Report:
               precision    recall  f1-score   support

           0      0.647     0.579     0.611        19
           1      0.385     0.238     0.294        21
           2      0.261     0.316     0.286        19
           3      0.160     0.267     0.200        15
           4      0.400     0.182     0.250        22
           5      0.429     0.562     0.486        16
           6      0.500     0.412     0.452        17
           7      0.364     0.364     0.364        22
           8      0.368     0.333     0.350        21
           9      0.000     0.000     0.000        19
          10      0.300     0.923     0.453        13

    accuracy                          0.358       204
   macro avg      0.347     0.380     0.341       204
weighted avg      0.351     0.358     0.335       204


class EEG_BiLSTMClassifier(nn.Module):
    def __init__(self, input_size=32, hidden_size=32, num_layers=2, num_classes=11, dropout=0.3):
        super(EEG_BiLSTMClassifier, self).__init__()

        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout,
            bidirectional=True  # ✅ BiLSTM enabled
        )

        # Note: hidden_size is doubled because it's bidirectional
        self.fc = nn.Sequential(
            nn.Linear(hidden_size * 2, 32),  # ✅ multiply by 2
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, num_classes)
        )

    def forward(self, x):
        # x: (batch_size, time_steps, features)
        lstm_out, _ = self.lstm(x)  # shape: (batch, time_steps, hidden_size * 2)

        # Take output from last time step
        last_output = lstm_out[:, -1, :]  # shape: (batch, hidden_size * 2)

        out = self.fc(last_output)  # shape: (batch, num_classes)
        return out


Features:
alpha, beta, mobility, rms, complexity, skewness, sampen, kurt