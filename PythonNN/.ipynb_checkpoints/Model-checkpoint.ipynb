{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7276cea1-fcd3-4d7d-9440-faee6e994788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "#from PyEMD import EMD\n",
    "\n",
    "#import antropy as ant\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2816cffe-f3f1-4eb5-bd73-39069a4dc8f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e19e47-53b0-4ab1-a9e6-621300da36f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "import numpy as np\n",
    "\n",
    "def compute_bandpower(data, fs, band):\n",
    "    freqs, psd = welch(\n",
    "        data,\n",
    "        fs=fs,\n",
    "        window='hamming',\n",
    "        nperseg=len(data),\n",
    "        scaling='density'\n",
    "    )\n",
    "    idx_band = np.logical_and(freqs >= band[0], freqs <= band[1])\n",
    "    return np.trapz(psd[idx_band], freqs[idx_band])\n",
    "\n",
    "def compute_rms(x):\n",
    "    return np.sqrt(np.mean(np.square(x)))\n",
    "\n",
    "def compute_hjorth_params(x):\n",
    "    first_deriv = np.diff(x)\n",
    "    second_deriv = np.diff(first_deriv)\n",
    "\n",
    "    var_zero = np.var(x)\n",
    "    var_d1 = np.var(first_deriv)\n",
    "    var_d2 = np.var(second_deriv)\n",
    "\n",
    "    mobility = np.sqrt(var_d1 / var_zero) if var_zero != 0 else 0\n",
    "    complexity = np.sqrt(var_d2 / var_d1) if var_d1 != 0 else 0\n",
    "\n",
    "    return mobility, complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d952838-1b47-4eae-8593-b0cd2c56bd14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_signal(signal):\n",
    "    max_abs = np.max(np.abs(signal))\n",
    "    return signal / max_abs if max_abs != 0 else signal\n",
    "\n",
    "class EEGWindowDataset(Dataset):\n",
    "    def __init__(self, combined_data_path, training_data_path, sampling_rate=250, window_sec=2):\n",
    "        self.combined_data = pd.read_csv(combined_data_path).values.astype(np.float32)\n",
    "        self.training_data = pd.read_csv(training_data_path).values.astype(np.int64)\n",
    "\n",
    "        self.window_size = int(window_sec * sampling_rate)\n",
    "        self.half_window = self.window_size // 2\n",
    "\n",
    "        self.total_rows = self.combined_data.shape[0]\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        class_label, center_idx = self.training_data[idx]\n",
    "\n",
    "        # Define full 2-second window\n",
    "        max_shift = int(0.5 * self.sampling_rate)  # 125 if sampling_rate = 250\n",
    "        random_shift = random.randint(-max_shift, max_shift)\n",
    "        \n",
    "        center_idx += random_shift\n",
    "        \n",
    "        start_idx = center_idx - self.half_window\n",
    "        end_idx = center_idx + self.half_window\n",
    "\n",
    "        # Edge padding if needed\n",
    "        if start_idx < 0 or end_idx > self.total_rows:\n",
    "            window = np.zeros((self.window_size, 4), dtype=np.float32)\n",
    "            actual_start = max(0, start_idx)\n",
    "            actual_end = min(self.total_rows, end_idx)\n",
    "            window_offset_start = max(0, -start_idx)\n",
    "            window[window_offset_start:window_offset_start + (actual_end - actual_start)] = \\\n",
    "                self.combined_data[actual_start:actual_end, :4]\n",
    "        else:\n",
    "            window = self.combined_data[start_idx:end_idx, :4]\n",
    "\n",
    "        # Split window into 4 segments (0.5s each @ 250 Hz = 125 samples)\n",
    "        segment_size = self.window_size // 5  # 0.5 seconds = 125 samples\n",
    "        features_per_segment = []\n",
    "        \n",
    "        #max_abs = np.max(np.abs(window))\n",
    "        #window = window / max_abs if max_abs != 0 else window\n",
    "\n",
    "        for i in range(5):  # 4 segments\n",
    "            segment = window[i * segment_size: (i + 1) * segment_size, :]\n",
    "            segment_features = []\n",
    "\n",
    "            for ch in range(4):  # 4 channels\n",
    "                signal = segment[:, ch]\n",
    "                \n",
    "                #band1 = compute_bandpower(signal, fs=self.sampling_rate, band=(8, 10))\n",
    "                #band2 = compute_bandpower(signal, fs=self.sampling_rate, band=(10, 13))\n",
    "                #band3 = compute_bandpower(signal, fs=self.sampling_rate, band=(13, 18))\n",
    "                #band4 = compute_bandpower(signal, fs=self.sampling_rate, band=(18, 24))\n",
    "                #band5 = compute_bandpower(signal, fs=self.sampling_rate, band=(24, 30))\n",
    "                #skewness = skew(signal)\n",
    "                #sampen = ant.sample_entropy(signal, order=2, metric='chebyshev')\n",
    "                #kurt = kurtosis(signal, fisher=True, bias=False)\n",
    "                \n",
    "                alpha = compute_bandpower(signal, fs=self.sampling_rate, band=(8, 13))\n",
    "                beta = compute_bandpower(signal, fs=self.sampling_rate, band=(13, 30))\n",
    "                rms = compute_rms(signal)\n",
    "                mobility, complexity = compute_hjorth_params(signal)\n",
    "                \n",
    "\n",
    "                segment_features.extend([alpha, beta, mobility, rms, complexity])\n",
    "                #segment_features.extend([band1, band2, band3, band4,band5])\n",
    "                #segment_features.extend([band4, band5, mobility, complexity])\n",
    "                \n",
    "            segment_features = np.array(segment_features, dtype=np.float32)\n",
    "                \n",
    "            #mu = np.mean(segment_features)\n",
    "            #sigma = np.std(segment_features)\n",
    "            #segment_features = (segment_features - mu) / sigma if sigma != 0 else segment_features\n",
    "\n",
    "            features_per_segment.append(segment_features)  # shape: (20,)\n",
    "            \n",
    "        features_per_segment = np.array(features_per_segment, dtype=np.float32)  # shape: (4, 20)\n",
    "\n",
    "        # Normalize across the entire 2s window (all segments together)\n",
    "        mu = np.mean(features_per_segment)\n",
    "        sigma = np.std(features_per_segment)\n",
    "        features_per_segment = (features_per_segment - mu) / sigma if sigma != 0 else features_per_segment\n",
    "\n",
    "        feature_tensor = torch.tensor(features_per_segment, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        #features_per_segment = np.array(features_per_segment, dtype=np.float32)  # ✅ efficient conversion\n",
    "        #feature_tensor = torch.tensor(features_per_segment, dtype=torch.float32)  # shape: (4, 20)\n",
    "        return feature_tensor, class_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f58d2e2-c45e-4ac7-86ba-ce9981b3c679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EEGDatasetCNNLSTM(Dataset):\n",
    "    def __init__(self, combined_data_path, training_data_path, sampling_rate=250, window_sec=2):\n",
    "        self.combined_data = pd.read_csv(combined_data_path).values.astype(np.float32)\n",
    "        self.training_data = pd.read_csv(training_data_path).values.astype(np.int64)\n",
    "\n",
    "        self.window_size = int(window_sec * sampling_rate)\n",
    "        self.half_window = self.window_size // 2\n",
    "\n",
    "        self.total_rows = self.combined_data.shape[0]\n",
    "        self.sampling_rate = sampling_rate\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        class_label, center_idx = self.training_data[idx]\n",
    "\n",
    "        # Define full 2-second window with random shift\n",
    "        max_shift = int(0.5 * self.sampling_rate)\n",
    "        random_shift = random.randint(-max_shift, max_shift)\n",
    "        center_idx += random_shift\n",
    "\n",
    "        start_idx = center_idx - self.half_window\n",
    "        end_idx = center_idx + self.half_window\n",
    "\n",
    "        # Edge padding if needed\n",
    "        if start_idx < 0 or end_idx > self.total_rows:\n",
    "            window = np.zeros((self.window_size, 4), dtype=np.float32)\n",
    "            actual_start = max(0, start_idx)\n",
    "            actual_end = min(self.total_rows, end_idx)\n",
    "            window_offset_start = max(0, -start_idx)\n",
    "            window[window_offset_start:window_offset_start + (actual_end - actual_start)] = \\\n",
    "                self.combined_data[actual_start:actual_end, :4]\n",
    "        else:\n",
    "            window = self.combined_data[start_idx:end_idx, :4]\n",
    "\n",
    "        # Transpose to (channels, time) shape expected by CNN\n",
    "        window = window.T  # shape: (4, 500)\n",
    "\n",
    "        # Normalize each channel independently across time\n",
    "        mean = np.mean(window, axis=1, keepdims=True)     # shape: (4, 1)\n",
    "        std = np.std(window, axis=1, keepdims=True)       # shape: (4, 1)\n",
    "        window = (window - mean) / (std + 1e-6)            # small value to avoid division by 0\n",
    "\n",
    "        # Convert to tensor\n",
    "        feature_tensor = torch.tensor(window, dtype=torch.float32)\n",
    "        return feature_tensor, class_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab962d-db0e-4c3d-8c02-1d2e80b2417c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c9d343f-fa78-4a25-8270-209a57ba4bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EEG_BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size=32, hidden_size=32, num_layers=2, num_classes=11, dropout=0.3):\n",
    "        super(EEG_BiLSTMClassifier, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True  # ✅ BiLSTM enabled\n",
    "        )\n",
    "\n",
    "        # Note: hidden_size is doubled because it's bidirectional\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 32),  # ✅ multiply by 2\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, time_steps, features)\n",
    "        lstm_out, _ = self.lstm(x)  # shape: (batch, time_steps, hidden_size * 2)\n",
    "\n",
    "        # Take output from last time step\n",
    "        last_output = lstm_out[:, -1, :]  # shape: (batch, hidden_size * 2)\n",
    "\n",
    "        out = self.fc(last_output)  # shape: (batch, num_classes)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30a0624-8cec-42af-93d1-bde3fbfcc935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EEG_LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size=20, hidden_size=32, num_layers=2, num_classes=11, dropout=0.3):\n",
    "        super(EEG_LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, time_steps, features)\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out: (batch, time_steps, hidden_size)\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        last_output = lstm_out[:, -1, :]  # shape: (batch_size, hidden_size)\n",
    "\n",
    "        normalized_output = self.bn(last_output)       # apply batch norm\n",
    "        out = self.fc(normalized_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd4142cf-3d79-461d-b915-8ef904b90ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNNLSTMClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super(CNNLSTMClassifier, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(4, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=32, batch_first=True)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(32)  # Match hidden size\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, time) → (B, 4, 500)\n",
    "        x = self.cnn(x)  # → (B, 64, time/4)\n",
    "        x = x.permute(0, 2, 1)  # → (B, time/4, 64) for LSTM\n",
    "        _, (hn, _) = self.lstm(x)        # hn: (1, B, 32)\n",
    "        hn = hn.squeeze(0)               # → (B, 32)\n",
    "        hn = self.bn(hn)                 # BatchNorm on final state\n",
    "        return self.classifier(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831a5c44-11ad-457b-a061-1f6819f7f66b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNNBiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super(CNNBiLSTMClassifier, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(4, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=32, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(32 * 2)  # Match hidden size\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32 * 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)  # → (B, 64, T)\n",
    "        x = x.permute(0, 2, 1)  # (B, T, 64)\n",
    "\n",
    "        _, (hn, _) = self.lstm(x)  # hn: (2, B, 32) if bidirectional=True\n",
    "        hn = torch.cat((hn[0], hn[1]), dim=1)  # (B, 64)\n",
    "\n",
    "        hn = self.bn(hn)  # BatchNorm on full concatenated hidden state\n",
    "        return self.classifier(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de373e6c-e6d5-4776-a810-bbdd65409d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MultiResBlock, self).__init__()\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=2, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=8, stride=2, padding=3),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run parallel conv branches\n",
    "        x1 = self.branch1(x)  # (B, C, T//2)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        min_len = min(x1.shape[2], x2.shape[2], x3.shape[2])\n",
    "        x1 = x1[:, :, :min_len]\n",
    "        x2 = x2[:, :, :min_len]\n",
    "        x3 = x3[:, :, :min_len]\n",
    "        return torch.cat([x1, x2, x3], dim=1)  # (B, C*3, T//2)\n",
    "\n",
    "class CNN2BlockBiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super(CNN2BlockBiLSTMClassifier, self).__init__()\n",
    "\n",
    "        # Block 1: Input (B, 4, 500) → (B, 96, 250)\n",
    "        self.block1 = MultiResBlock(in_channels=4, out_channels=32)\n",
    "\n",
    "        # Block 2: Input (B, 96, 250) → (B, 96, 125)\n",
    "        self.block2 = MultiResBlock(in_channels=96, out_channels=64)\n",
    "\n",
    "        # LSTM input: (B, 125, 96*2)\n",
    "        self.lstm = nn.LSTM(input_size=192, hidden_size=150,num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(300)  # 32 * 2 from BiLSTM\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(300, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)               # (B, 96, 250)\n",
    "        x = self.block2(x)               # (B, 96, 125)\n",
    "        x = x.permute(0, 2, 1)           # (B, 125, 96) → time-major for LSTM\n",
    "\n",
    "        _, (hn, _) = self.lstm(x)        # hn: (2, B, 32)\n",
    "        hn = torch.cat((hn[0], hn[1]), dim=1)  # (B, 64)\n",
    "\n",
    "        hn = self.bn(hn)\n",
    "        return self.classifier(hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d5e90-544b-4e43-b235-010ecbc96aac",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffd52e25-0aca-4ccc-80c5-74f65ea833b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=30, device='cuda'):\n",
    "    model.to(device)\n",
    "    count = 0\n",
    "    epoch_bar = trange(num_epochs, desc=\"Training Epochs\", unit=\"epoch\")\n",
    "    \n",
    "    for epoch in epoch_bar:\n",
    "        # -------- Training Phase --------\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "\n",
    "            # Compute individual losses\n",
    "            criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "            losses = criterion(outputs, batch_y)  # shape: (batch_size,)\n",
    "\n",
    "            loss = losses.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Update progress bar with loss\n",
    "        epoch_bar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        epoch_bar.set_postfix(loss=avg_loss)\n",
    "\n",
    "    # -------- Validation Phase (at end of training) --------\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_X, val_y in val_loader:\n",
    "            val_X, val_y = val_X.to(device), val_y.to(device)\n",
    "            outputs = model(val_X)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            y_true.extend(val_y.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    val_accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=3))\n",
    "    \n",
    "    return val_accuracy, avg_loss, cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ecaa5-4fee-4b46-8130-938b324ec91d",
   "metadata": {},
   "source": [
    "## Window Size Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41dfc6f4-7e62-446b-ab1e-c5ed1ce997be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training for Window Size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000:   5%|██▍                                                 | 48/1000 [02:17<45:24,  2.86s/epoch, loss=1.1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m train_loader = DataLoader(training_dataset, batch_size=\u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     22\u001b[39m val_loader = DataLoader(testing_dataset, batch_size=\u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m acc, loss, cm = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m cm_array.append(cm)\n\u001b[32m     26\u001b[39m accuracies.append(acc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, criterion, num_epochs, device)\u001b[39m\n\u001b[32m     19\u001b[39m losses = criterion(outputs, batch_y)  \u001b[38;5;66;03m# shape: (batch_size,)\u001b[39;00m\n\u001b[32m     21\u001b[39m loss = losses.mean()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m optimizer.step()\n\u001b[32m     25\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "window_sizes = [2]\n",
    "cm_array = []\n",
    "accuracies = []\n",
    "losses = []\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    print(\"Training for Window Size:\", window_size)\n",
    "\n",
    "    model = CNNBiLSTMClassifier()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    #criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    \n",
    "    training_dataset = EEGDatasetCNNLSTM(\"ShawnAndNickTrainingData/combined_data.csv\", \"ShawnAndNickTrainingData/training_data.csv\", sampling_rate=250, window_sec=window_size)\n",
    "    testing_dataset = EEGDatasetCNNLSTM(\"ShawnAndNickTrainingData/combined_data.csv\", \"ShawnAndNickTrainingData/testing_data.csv\", sampling_rate=250, window_sec=window_size)\n",
    "\n",
    "    train_loader = DataLoader(training_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(testing_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    acc, loss, cm = train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=1000, device=device)\n",
    "    cm_array.append(cm)\n",
    "    accuracies.append(acc)\n",
    "    losses.append(loss)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Accuracy bar plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar([str(bs) for bs in window_sizes], accuracies, color='skyblue')\n",
    "plt.title(\"Validation Accuracy vs Batch Size\")\n",
    "plt.xlabel(\"Window Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, axis='y')\n",
    "\n",
    "# Loss bar plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar([str(bs) for bs in window_sizes], losses, color='salmon')\n",
    "plt.title(\"Validation Loss vs Batch Size\")\n",
    "plt.xlabel(\"Window Size\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7dd31277-b9da-40ca-bfd4-e772d09ac8ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "fe4e1346-6ea2-4208-bde0-68c2bb4a816b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, marker='o', label='Training Loss')\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fac52c98-6aa9-4186-bd7e-b3ca6c6219ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  1  7  0  0  0  0  1  0  0  0]\n",
      " [ 0  7 10  0  0  2  0  0  0  2  0]\n",
      " [ 0  7  5  0  0  3  0  0  0  4  0]\n",
      " [ 0  0  0  9  0  1  3  1  1  0  0]\n",
      " [ 0  0  0 11  0  2  8  0  1  0  0]\n",
      " [ 0  0  0  1  0 10  4  0  0  1  0]\n",
      " [ 0  0  0  6  0  5  6  0  0  0  0]\n",
      " [ 2  0  0  4  0  1  8  6  1  0  0]\n",
      " [ 0  0  0  5  0  0  7  7  2  0  0]\n",
      " [ 0  2  0  0  0  0  0  0  0 15  2]\n",
      " [ 0  0  0  0  1  0  0  0  0 12  0]]\n",
      "Accuracy:  0.3431372549019608\n",
      "Loss:  1.5297337495363676\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cm_array)):\n",
    "    print(cm_array[i])\n",
    "    print(\"Accuracy: \", accuracies[i])\n",
    "    print(\"Loss: \", losses[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127df8cb-6587-43ff-8b99-05842045d13e",
   "metadata": {},
   "source": [
    "## Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1e5494c-d8c1-4455-895b-f1afa8479b44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 20, got 500",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m val_X, val_y \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[32m     15\u001b[39m     val_X, val_y = val_X.to(\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m), val_y.to(\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     outputs = \u001b[43mtorch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     preds = torch.argmax(outputs, dim=\u001b[32m1\u001b[39m)\n\u001b[32m     19\u001b[39m     y_true.extend(val_y.cpu().numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mEEG_LSTMClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# x: (batch_size, time_steps, features)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     lstm_out, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# lstm_out: (batch, time_steps, hidden_size)\u001b[39;00m\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Take the output from the last time step\u001b[39;00m\n\u001b[32m     26\u001b[39m     last_output = lstm_out[:, -\u001b[32m1\u001b[39m, :]  \u001b[38;5;66;03m# shape: (batch_size, hidden_size)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1101\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1093\u001b[39m     c_zeros = torch.zeros(\n\u001b[32m   1094\u001b[39m         \u001b[38;5;28mself\u001b[39m.num_layers * num_directions,\n\u001b[32m   1095\u001b[39m         max_batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1098\u001b[39m         device=\u001b[38;5;28minput\u001b[39m.device,\n\u001b[32m   1099\u001b[39m     )\n\u001b[32m   1100\u001b[39m     hx = (h_zeros, c_zeros)\n\u001b[32m-> \u001b[39m\u001b[32m1101\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1002\u001b[39m, in \u001b[36mLSTM.check_forward_args\u001b[39m\u001b[34m(self, input, hidden, batch_sizes)\u001b[39m\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_forward_args\u001b[39m(\n\u001b[32m    997\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    998\u001b[39m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[32m    999\u001b[39m     hidden: Tuple[Tensor, Tensor],  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m   1000\u001b[39m     batch_sizes: Optional[Tensor],\n\u001b[32m   1001\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_hidden_size(\n\u001b[32m   1004\u001b[39m         hidden[\u001b[32m0\u001b[39m],\n\u001b[32m   1005\u001b[39m         \u001b[38;5;28mself\u001b[39m.get_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[32m   1006\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1007\u001b[39m     )\n\u001b[32m   1008\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_hidden_size(\n\u001b[32m   1009\u001b[39m         hidden[\u001b[32m1\u001b[39m],\n\u001b[32m   1010\u001b[39m         \u001b[38;5;28mself\u001b[39m.get_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[32m   1011\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1012\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:314\u001b[39m, in \u001b[36mRNNBase.check_input\u001b[39m\u001b[34m(self, input, batch_sizes)\u001b[39m\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    311\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.dim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    312\u001b[39m     )\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.input_size != \u001b[38;5;28minput\u001b[39m.size(-\u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    315\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.input_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size(-\u001b[32m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    316\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: input.size(-1) must be equal to input_size. Expected 20, got 500"
     ]
    }
   ],
   "source": [
    "model_path = \"models/LSTM_0.0.4/model.pth\"\n",
    "#torch_model = CNNBiLSTMClassifier()\n",
    "#torch_model = CNN2BlockBiLSTMClassifier()\n",
    "torch_model = EEG_LSTMClassifier()\n",
    "torch_model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "torch_model.eval()\n",
    "\n",
    "testing_dataset = EEGWindowDataset(\"FromHandOpenTesting/combined_data.csv\", \"FromHandOpenTesting/testing_data.csv\", sampling_rate=250, window_sec=2)\n",
    "test_loader = DataLoader(testing_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_X, val_y in test_loader:\n",
    "        val_X, val_y = val_X.to('cpu'), val_y.to('cpu')\n",
    "        outputs = torch_model(val_X)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        y_true.extend(val_y.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "val_accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba393e4-a23b-4209-8591-34b7fdef2c7d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
